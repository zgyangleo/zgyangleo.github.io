---
layout: post
title:  "线性回归"
date:   2019-7-31 20:23:01 +0800
categories: 机器学习
tag: 线性回归
---

* content
{:toc}

[线性回归（Linear regression)](#)

线性回归是在平面中根据所给的点求出一条直线， 并且使得所有的点到这条线的距离平方和最小。

[一元线性回归](#)

线性回归试图学得f(x) = ax + b, 需要衡量f(x)和y之间的区别。均方误差是回归任务中常用的方法，我们的目的就是让均方误差最小化，即：

![1]({{ '/styles/images/20190731/20180607154726515.gif' | prepend: site.baseurl  }})

均方误差对应了“欧式距离”。基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。下面是简单的推导过程：、

![2]({{ '/styles/images/20190731/20180607154805966.gif' | prepend: site.baseurl  }}) 

分别对a和b求导

![3]({{ '/styles/images/20190731/20180607155455599.gif' | prepend: site.baseurl  }}) 

![4]({{ '/styles/images/20190731/20180607155811900.gif' | prepend: site.baseurl  }}) 

偏导为0, 可得：

![5]({{ '/styles/images/20190731/20180607160707243.gif' | prepend: site.baseurl  }}) 

![6]({{ '/styles/images/20190731/20180607160906359.gif' | prepend: site.baseurl  }}) 

其中  ![7]({{ '/styles/images/20190731/2018060716110573.gif' | prepend: site.baseurl  }})  为已知x的均值。

[多元线性回归](#)

多元线性回归是指有多个影响因素决定结果。用函数表示为 ![7]({{ '/styles/images/20190731/20180607234329881.gif' | prepend: site.baseurl  }}) 

用向量表示方程为 

![8]({{ '/styles/images/20190731/20180607234208728.gif' | prepend: site.baseurl  }}) 

再把样本表示为向量形式 ![8]({{ '/styles/images/20190731/20180607234208728.gif' | prepend: site.baseurl  }}) 根据前面的均方差， 可得：

![9]({{ '/styles/images/20190731/20180607235538440.gif' | prepend: site.baseurl  }}) 

令上式为0,可得：

![8]({{ '/styles/images/20190731/20180607235840226.gif' | prepend: site.baseurl  }})  此时![8]({{ '/styles/images/20190731/20180608000118461.gif' | prepend: site.baseurl  }})  必须满足可逆的， 即满秩的或者正定矩阵。 然而现实任务中这个条件很难满足。我们可以通过引入正则项来解决。





